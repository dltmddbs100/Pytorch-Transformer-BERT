A Pytorch Implementation of the Transformer Network
---
**Transformer**

Pytorch implementations of "Attention is All You Need" (Vaswani et al., NIPS 2017) and "Weighted Transformer Network for Machine Translation" (Ahmed et al., arXiv 2017)

**BERT**

Pytorch implementations of "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin, Jacob, et al., arXiv 2018)

Reference
---
**Paper**

- Vaswani et al., "Attention is All You Need", NIPS 2017
- Ahmed et al., "Weighted Transformer Network for Machine Translation", Arxiv 2017
- Devlin, Jacob, et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"

**Code**

- Annotated Transformer - https://nlp.seas.harvard.edu/2018/04/03/attention.html
- huggingface BERT - https://huggingface.co/docs/transformers/model_doc/bert
- huggingface github - https://github.com/huggingface/transformers
